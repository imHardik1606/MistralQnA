# ğŸ“„ PDF Chatbot using Mistral (RAG)

A simple Retrieval-Augmented Generation (RAG) application that allows users to ask
questions about a PDF document and receive grounded answers using **Mistral models**.

This project is built as a **developer-facing AI tool** with a focus on correctness,
clarity, and best engineering practices rather than UI complexity.

---

#### Built as part of an internship application to demonstrate Mistral SDK usage

---

## ğŸš€ Why this project?

Large language models cannot reliably answer questions about long documents without
external context. This project demonstrates how **Retrieval-Augmented Generation (RAG)** can:

- Ground model responses in real document content
- Reduce hallucinations
- Make LLM behavior transparent and debuggable

The project is intentionally kept simple to highlight **system design decisions**
and **LLM integration best practices**.

---

## ğŸ§  Architecture Overview (Simple Explanation)

1. A PDF is uploaded via the UI
2. The document is split into overlapping text chunks
3. Each chunk is converted into an embedding
4. Embeddings are stored in a vector store (FAISS)
5. When a user asks a question:
   - The question is embedded
   - Relevant chunks are retrieved
   - Retrieved chunks are passed as context to a Mistral model
6. The model generates an answer strictly based on the retrieved context

---
## ğŸ¤– Mistral AI Models Used

This application leverages Mistral AI's specialized models for optimal RAG performance:

### **Configuration (`app/config.py`)**
```python
# Core Mistral models
CHAT_MODEL = "mistral-small-latest"  # For answer generation
EMBED_MODEL = "mistral-embed"        # For text embeddings

# RAG parameters
CHUNK_SIZE = 1000     # Characters per text chunk
CHUNK_OVERLAP = 200   # Overlap between chunks for context preservation
TOP_K = 2            # Number of chunks to retrieve per question
```
---

## ğŸ”„ Application Flow (Flowchart)

```mermaid
flowchart TD
    A[Upload PDF] --> B[Extract Text]
    B --> C[Chunk Text]
    C --> D[Create Embeddings]
    D --> E[Store in Vector Store]

    F[User Question] --> G[Embed Question]
    G --> H[Retrieve Relevant Chunks]
    H --> I[Send Context + Question to Mistral]
    I --> J[Generate Answer]
    J --> K[Display Answer + Retrieved Chunks]
````

---

## ğŸ—‚ï¸ Project Structure

```
pdf-chatbot-mistral/
â”œâ”€â”€ app/                    # Core application
â”‚   â”œâ”€â”€ config.py          # Settings & constants
â”‚   â”œâ”€â”€ mistral_client.py  # Mistral API wrapper
â”‚   â”œâ”€â”€ rag.py             # Chunking & retrieval logic
â”‚   â””â”€â”€ ui.py              # Streamlit UI helpers
â”œâ”€â”€ tests/                  # Comprehensive test suite
â”œâ”€â”€ main.py                # Application entry point
â”œâ”€â”€ requirements.txt       # Production dependencies
â”œâ”€â”€ requirements-dev.txt   # Development tools
â””â”€â”€ README.md             # You're reading it!
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/imHardik1606/pdf-chatbot-mistral.git
cd pdf-chatbot-mistral
```

### 2ï¸âƒ£ Create and activate a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set environment variables

Create a `.env` file in the root directory:

```env
MISTRAL_API_KEY=your_api_key_here
```

---

## â–¶ï¸ Running the Application

```bash
streamlit run main.py
```
Open http://localhost:8501 and start chatting with PDFs! ğŸ“„

Once running:

* Upload a PDF
* Wait for chunking and embedding to complete
* Ask questions about the document

---

## ğŸ§ª Testing (Made Easy)

We've included a complete test suite so you can verify everything works:

```bash
# Install test tools (once)
pip install -r requirements-dev.txt

# Run all tests
pytest tests/

# Expected: 12 out of 14 tests pass âœ…
# The 2 "failing" tests are edge cases we've documented

# See what's tested
pytest tests/ -v

# Check code coverage
pytest tests/ --cov=app --cov-report=term-missing
```

### What We Test:
- âœ… **Text chunking** - Splitting documents intelligently
- âœ… **FAISS operations** - Vector search works correctly  
- âœ… **API client** - Mocked Mistral API calls
- âœ… **PDF processing** - Text extraction from PDFs
- âœ… **Edge cases** - Empty docs, small files, etc.

### Test Structure:
```
tests/
â”œâ”€â”€ conftest.py           # Contains chunks of text
â”œâ”€â”€ diagnostic.py          # Diagnose the test suite 
â”œâ”€â”€ test_rag.py           # Core RAG logic tests
â”œâ”€â”€ test_mistral_client.py # API integration tests  
â””â”€â”€ test_ui.py           # UI/PDF processing tests
```

---

Thatâ€™s a **very good instinct** â€” and youâ€™re right.
If the reviewer doesnâ€™t know *which* PDF you used, **example questions tied to a specific book are confusing** and slightly unprofessional.

What you want instead is **question *types***, not question *content*.

Below is a **replacement section** you can drop into `README.md`.
Itâ€™s generic, reviewer-friendly, and reads like something an engineer at Mistral would write.

---

## ğŸ§© What Kind of Questions Should Be Asked?

This application uses a **Retrieval-Augmented Generation (RAG)** pipeline that answers
questions strictly based on retrieved text from the uploaded document.

As a result, performance depends heavily on the **structure of the question**.

---

### âœ… Well-Supported Question Types

The system performs best on questions where the answer is **explicitly present**
within a limited portion of the document:

* **Factual questions**

  * Asking about concrete information stated in the text

* **Definition or description questions**

  * Asking how an entity, concept, or event is described

* **Local context questions**

  * Asking about content from a specific section or part of the document

* **Single-hop questions**

  * Questions that can be answered without reasoning across distant sections

These questions align well with the retrieval step and usually result in grounded,
verifiable answers.

---

### âš ï¸ Question Types With Known Limitations

The following types of questions may produce incomplete or unreliable answers:

* **Global summarization**

  * Questions requiring understanding of the entire document

* **Multi-hop reasoning**

  * Questions that depend on connecting information across many sections

* **Abstract or interpretive questions**

  * Questions that require inference beyond what is explicitly written

* **Timeline-wide or narrative arc questions**

  * Questions spanning large portions of long documents

These limitations are expected in a basic RAG system without hierarchical retrieval
or long-context reasoning.

---

### ğŸ§ª How to Evaluate Answer Quality

To assess whether the system is working correctly:

1. Ask a factual or locally scoped question
2. Inspect the retrieved chunks displayed in the UI
3. Confirm that the answer is derived from the retrieved text

Answers that cannot be traced back to retrieved content should be treated cautiously.

---

### â„¹ï¸ Why These Constraints Exist

This system intentionally:

* Uses fixed-size chunking
* Retrieves a limited number of chunks per query
* Avoids document-wide reasoning for transparency

These tradeoffs keep the system **simple, debuggable, and aligned with RAG best practices**.

---

### âœ… Why this section helps reviewers

* It shows you **understand RAG limitations**
* It sets correct expectations
* It avoids dataset-specific assumptions
* It demonstrates engineering maturity

## âš ï¸ Limitations (Important)

* The system can only answer questions explicitly present in the document
* Narrative or global questions (e.g. *â€œWhat happens at the end?â€*) may fail on very large PDFs
* No chapter-level or section metadata is used
* The model does not reason beyond retrieved chunks
* Chunk size and overlap are fixed and may not be optimal for all documents

These limitations are expected for a basic RAG pipeline and are documented intentionally.

---

## ğŸ” Design Decisions

* **Streamlit** was chosen for fast prototyping and easy testing of retrieval behavior
* Core logic is separated from UI for clarity and maintainability
* A Mistral SDK wrapper isolates model-specific code
* Emphasis is on transparency and correctness rather than UI complexity

---

## ğŸ”® Possible Improvements

* Add page or chapter-level metadata
* Display citations with answers
* Support multiple documents
* Use hierarchical chunking for large PDFs
* Add evaluation metrics for retrieval quality

---

## ğŸ¤– Model Usage

* **Embeddings**: Mistral-compatible embedding model
* **Generation**: Mistral small model (chosen for fast iteration and cost efficiency)

The project focuses on **system design and reliability**, not model size.

---

## ğŸ§ª How to Test Correctness

* Ask factual questions grounded in the text
* Inspect retrieved chunks shown in the UI
* Verify answers are derived from retrieved content

---

## ğŸ“Œ Final Notes

This project is intended as a **technical demonstration** of building AI-powered
developer tools using Mistral models. It is not production-ready but follows
industry best practices for prototyping and experimentation.